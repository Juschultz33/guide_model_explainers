<!DOCTYPE html>
<html>
  <head>
    <title>My Awesome Presentation</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style type="text/css">
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
      
      body { font-family: 'Droid Serif'; }
      h1 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
        color:darkslategrey;
      }
      h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .my-one-page-font {
        font-size: 30px;
      }     
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
            /* Two-column layout */
      .left-column {
        color: #777;
        width: 30%;
        float: left;
      }
        .left-column h2:last-of-type, .left-column h3:last-child {
          color: #000;
        }
      .right-column {
        width: 70%;
        float: right;
        padding-top: 1em;
      }
      .inverse {
        background: #272822;
        color: #e4e4e1;
        text-shadow: 0 0 20px #333;
      }
      .inverse h1, .inverse h2, .inverse h3 {
        color: #f3f3f3;
        line-height: 0.8em;
      }
      .lightfont {color:rgb(129, 126, 126);
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Model Explainers in PySpark

checkout the slides branch for a pdf.

---

class: center, middle

# What is a model explainer?

???

Model explainers help explain the "black box" that are machine learning models.
As we've been going in class, we've been generating hundreds of features. But training a model on so many 
features can be resource intensive. It can also take time to generate all those features. By figuring out the 
features that are most relevant to our model, we can focus on those. We could even drop our useless features 
(no correlation to the final output prediction) and generate more features that are like our most important features.
We can do this manually, but that takes a ton of time.

Another advantage to model explainers is understanding individual instances that our model predicts. 
In statistics, we can understand things "on the average". That is to say, if someone was to apply for a loan, 
we can say "because most people that are approved for a loan have a credit score of 750, and your credit score 
is 700, you're unlikely to be approved for a loan". However, if we had a model explainer for a machine learning 
model that decides if you are approved for a loan, we could look at each factor that changes the final outcome, 
and how much it changes it by. We'll show an example of this more in later sections



---

# What do we do with the info a model explainer gives us? 

Model explainers give us a good look into the black box and what is happening in the background of the 'black box'.
SHAP lets us look at individual SHAP values, and how values could affect the SHAP value. This could give us a look
into problems with our model that need to be corrected or if our features we have made have too much of an affect on 
our target variable. 
      
In the example of getting approval for a loan a feature like years of credit history might have a large affect on if someone
gets approved for a loan or not. However, it could also have an affect showing someones age which cannot be discrimiated againt
when applying for a loan.

---

# SHAP

SHAP is one of the best model explainers for Python (and really, most ML is done in Python).

SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. 
It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their 
related extensions. (Shapley values are a solution concept in cooperative game theory).

SHAP Values are used in the SHAP model explainer to show how each feature and value correlate the model output.

SHAP has the ability to show the same model summaries that the built in feature importance functions can show. 
However, the real power of model explainers comes in with the ability to show each individual observation.

We'll discuss this more as we get into some sample code.

---

# DALEX

---

# Let's look at the example code in Databricks

The link can be [here](https://adb-5187062830023627.7.azuredatabricks.net/?o=5187062830023627#notebook/4497061773294494/command/4497061773294495).  
Alternatively, you can find the link to the code in our GitHub template or in Databrick>Worspace>Shared>Team Presentations>Model Explainers

---

# Add more slides with '---'

---

# Fin

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      remark.macros.upper = function () {
        // `this` is the value in the parenthesis, or undefined if left out
        return this.toUpperCase();
      };

      remark.macros.random = function () {
        // params are passed as function arguments: ["one", "of", "these", "words"]
        var i = Math.floor(Math.random() * arguments.length);
        return arguments[i];
      };
      
      remark.macros.scale = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
      };
      
      var slideshow = remark.create({
        ratio: "16:9",
        highlightLanguage: 'javascript',
        highlightStyle: 'monokai'
       });
    </script>
  </body>
</html>
